<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Audio Flamingo 2</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f5f5f5;
            text-align: center;
        }
        header {
            background-color: #1a1a2e;
            color: white;
            text-align: center;
            padding: 20px;
        }
        .logo {
            max-width: 150px;
            margin-bottom: 10px;
        }
        .container {
            width: 80%;
            margin: auto;
            padding: 20px;
            text-align: left;
        }
        .section {
            background: white;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
            box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.1);
        }
        .section img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 10px auto;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 20px;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #1a1a2e;
            color: white;
        }
        footer {
            text-align: center;
            padding: 10px;
            background-color: #1a1a2e;
            color: white;
            margin-top: 20px;
        }
    </style>
    <script>
        async function loadExamples() {
    try {
        console.log("Fetching JSON data...");

        // Ensure the correct file path
        const response = await fetch('./af2/example.json');

        if (!response.ok) {
            throw new Error(`HTTP error! Status: ${response.status}`);
        }

        const data = await response.json();
        console.log("JSON data loaded successfully:", data);

        const tableBody = document.getElementById('examplesTableBody');
        if (!Array.isArray(data)) {
            throw new Error("JSON data is not an array");
        }

        data.forEach(item => {
            console.log("Processing item:", item);

            const row = document.createElement('tr');
            row.innerHTML = `
                <td>
                    <audio controls>
                        <source src="${item.audio}.wav" type="audio/mpeg">
                        Your browser does not support the audio element.
                    </audio>
                </td>
                <td>${item.question}</td>
                <td>${item.qwen2_audio_answer || 'N/A'}</td>
                <td>${item.gama_answer || 'N/A'}</td>
                <td>${item.af2_answer || 'N/A'}</td>
            `;
            tableBody.appendChild(row);
        });

        console.log("Table updated successfully!");
    } catch (error) {
        console.error("Error loading JSON:", error);
    }
}

// Run fetch function after window loads
window.onload = loadExamples;
    </script>
</head>
<body>
    <header>
        <img src="af2/af_logo.png" alt="Audio Flamingo 2 Logo" class="logo">
        <h1>Audio Flamingo 2</h1>
        <p>An Audio-Language Model with Long-Audio Understanding</p>
    </header>
        
    
            
            <div class="container">
                <div class="section">
                    <h2>Abstract</h2>
                    <img src="af2/af2_training_pipeline_old-1.png" alt="Illustration of Audio Flamingo 2" width="75%" class="center">
                    <p>Understanding and reasoning over non-speech sounds and music are crucial for both humans and AI agents to interact effectively with their environments. In this paper, we introduce \textbf{Audio Flamingo 2} (AF2), an Audio-Language Model (ALM) with advanced audio understanding and reasoning capabilities. AF2 leverages (i) a custom CLAP model, (ii) synthetic AQA data for fine-grained audio reasoning, and (iii) a multi-stage curriculum learning strategy. AF2 achieves state-of-the-art performance with only a 3B parameter small language model, surpassing large open-source and proprietary models across 20+ benchmarks. Next, for the first time, we extend audio understanding to long audio segments (30 secs - 5 mins) and propose \textbf{LongAudio}, a large and novel dataset for training ALMs on long audio captioning and question-answering tasks. Fine-tuning AF2 on LongAudio leads to exceptional performance on our proposed \textbf{LongAudioBench}, an expert annotated benchmark for evaluating ALMs on long audio understanding capabilities. We conduct extensive ablation studies to confirm the efficacy of our approach. All code and data will be open-sourced. </p>
                </div>
                <div class="section"> 
                    <h2>Overview</h2>
                    <p> Understanding non-speech sounds, non-verbal speech, and music is essential for real-world applications such as detecting anomalies in industrial environments, recognizing emotional cues, and improving assistive technologies for the impaired. While Large Language Models (LLM) have demonstrated remarkable reasoning capabilities through language, extending these systems to comprehend audio is key to building intelligent systems capable of reasoning with contextual auditory cues. Verbal speech, inherently tied to language, has benefited significantly from LLM advancements. However, the potential to enhance perception and reasoning over non-verbal audio remains largely under-explored. 
                    In this paper, we introduce Audio Flamingo 2, an Audio-Language Model with advanced long-audio understanding and reasoning capabilities. Audio Flamingo 2 achieves the state-of-the-art performance across over 20 benchmarks, with only a 3B parameter small language model.
                    <ul>
                        <li>We introduce two datasets, AudioSkills for expert audio reasoning, and LongAudio for long audio understanding, to advance this field.
                        <li>Audio Flamingo 2 has advanced audio understanding and reasoning capabilities. Especially, Audio Flamingo 2 has expert audio reasoning abilities, and can understand long audio up to 5 minuts.
                            <li>Audio Flamingo 2 outperforms larger and proprietary LALMs across 20+ benchmarks, despite being smaller (3B) and trained exclusively on public datasets.
                </ul>
                <img src="af2/radar_af2_new_norm_sk-1.png" alt="Illustration of Audio Flamingo 2" width="50%" class="center">
                </div>
            <!-- <div class="section">
                <h2>Key Features</h2>
                <ul>
                    <li>Advanced audio reasoning and understanding</li>
                    <li>Supports long-audio (30 sec - 5 min) analysis</li>
                    <li>Outperforms larger models across 20+ benchmarks</li>
                    <li>Uses a custom CLAP model with enhanced training</li>
                    <li>Open-source with a demo available</li>
                </ul>
            </div> -->
    
            <div class="section">
                <h2>AudioSkills Dataset Examples</h2>
                <img src="af2/audioskills_examples-1.png" alt="audioskills_exampls" width="75%" class="center">
            </div>
    
            <div class="section">
                <h2>AF-CLAP</h2>
                <p>
                Representations in current CLAP models struggle with compositional reasoning and linguistic variations in captions. We introduce an improved version of CLAP called AF-CLAP, where we (1) construct a large-scale, high-quality training dataset, and (2) improve the training objective to for better representational quality and robustness.
                For each audio-caption pair, we construct linguistically varied captions with identical semantics and composition, and regard these as additional positives. We then generate caption variations with modified temporal or attribute compositions, and regard these as additional negatives. Our improved contrastive loss considers these additional positives and negatives, which leads to more human-aligned representation and better results (in both representation learning and audio understanding).
            </p>
                <img src="af2/af2_clap-1.png" alt="CLAP" width="75%" class="center">
                <img src="af2/AFCLAP_retrieval.png" alt="CLAP" width="75%" class="center">
                <img src="af2/AF2_clap.png" alt="CLAP" width="50%" class="center">
                
            </div>

            <div class="section">
                <h2>Curriculum Training</h2>
                <p>Audio Flamingo 2 is trained with a 3-stage curriculum.</p>
                    <ul>
                    <li>We train transformation and cross attention layers on 30 seconds on the pre-training dataset.    
                    <li>Fine-tuning: we train all but LLM layers on 1.5 minutes on the fine-tuning dataset.
                    <li>Long fine-tuning: we train transformation and cross attention layers on 5 minutes on the LongAudio dataset.
                    <li>Additional ablations on the curriculum training can be found below.
                    </ul>
                <img src="af2/cc_train.png" alt="long audii" width="50%" class="center">
            </div>
    
            <div class="section">
                <h2>Long Audio Training Pipeline</h2>
                <img src="af2/long_audio_pipeline-1.png" alt="long audii" width="75%" class="center">
            </div>
    
            <div class="section">
                <h2>Benchmarks & Performance</h2>
                <p>AF2 achieves state-of-the-art accuracy across various benchmarks, including ClothoAQA, AudioCaps, MMAU, and LongAudioBench. It outperforms proprietary models while being significantly smaller.</p>
                <img src="af2/tab1.png" alt="Benchmark comparison graph">
                <img src="af2/tab2.png" alt="Performance evaluation chart" width="50%">
            </div>
        <div class="section">
            <h2>Examples</h2>
            <p>AF2 is trained on the novel <strong>LongAudio</strong> dataset, consisting of 260K+ AQA pairs, and <strong>LongAudioBench</strong>, an expert-verified evaluation benchmark for long audio understanding.</p>
            <table>
                <thead>
                    <tr>
                        <th>Audio</th>
                        <th>Question</th>
                        <th>Qwen2-Audio</th>
                        <th>GAMA</th>
                        <th>AudioFlamingo 2</th>
                    </tr>
                </thead>
                <tbody id="examplesTableBody">
                </tbody>
            </table>
        </div>
            
    <footer>
        <p>&copy; 2025 Audio Flamingo 2 | ICML Publication</p>
    </footer>
</body>
</html>
