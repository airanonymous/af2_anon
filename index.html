<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Audio Flamingo 2</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f5f5f5;
            text-align: center;
        }
        header {
            background-color: #1a1a2e;
            color: white;
            text-align: center;
            padding: 20px;
        }
        .logo {
            max-width: 150px;
            margin-bottom: 10px;
        }
        .container {
            width: 80%;
            margin: auto;
            padding: 20px;
            text-align: left;
        }
        .section {
            background: white;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
            box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.1);
        }
        .section img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 10px auto;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 20px;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #1a1a2e;
            color: white;
        }
        footer {
            text-align: center;
            padding: 10px;
            background-color: #1a1a2e;
            color: white;
            margin-top: 20px;
        }
    </style>
    <script>
        async function loadExamples() {
    try {
        console.log("Fetching JSON data...");

        // Ensure the correct file path
        const response = await fetch('./af2/example.json');

        if (!response.ok) {
            throw new Error(`HTTP error! Status: ${response.status}`);
        }

        const data = await response.json();
        console.log("JSON data loaded successfully:", data);

        const tableBody = document.getElementById('examplesTableBody');
        if (!Array.isArray(data)) {
            throw new Error("JSON data is not an array");
        }

        data.forEach(item => {
            console.log("Processing item:", item);

            const row = document.createElement('tr');
            row.innerHTML = `
                <td>
                    <audio controls>
                        <source src="${item.audio}.wav" type="audio/mpeg">
                        Your browser does not support the audio element.
                    </audio>
                </td>
                <td>${item.question}</td>
                <td>${item.qwen2_audio_answer || 'N/A'}</td>
                <td>${item.gama_answer || 'N/A'}</td>
                <td>${item.af2_answer || 'N/A'}</td>
            `;
            tableBody.appendChild(row);
        });

        console.log("Table updated successfully!");
    } catch (error) {
        console.error("Error loading JSON:", error);
    }
}

// Run fetch function after window loads
window.onload = loadExamples;
    </script>
</head>
<body>
    <header>
        <img src="af2/af_logo.png" alt="Audio Flamingo 2 Logo" class="logo">
        <h1>Audio Flamingo 2</h1>
        <p>An Audio-Language Model with Long-Audio Understanding</p>
    </header>
        
        <div class="container">
            <div class="section">
                <h2>Abstract</h2>
                <img src="af2/af2_training_pipeline_old-1.png" alt="Illustration of Audio Flamingo 2" width="75%" class="center">
                <p>Understanding and reasoning over non-speech sounds and music are crucial for both humans and AI agents to interact effectively with their environments. In this paper, we introduce \textbf{Audio Flamingo 2} (AF2), an Audio-Language Model (ALM) with advanced audio understanding and reasoning capabilities. AF2 leverages (i) a custom CLAP model, (ii) synthetic AQA data for fine-grained audio reasoning, and (iii) a multi-stage curriculum learning strategy. AF2 achieves state-of-the-art performance with only a 3B parameter small language model, surpassing large open-source and proprietary models across 20+ benchmarks. Next, for the first time, we extend audio understanding to long audio segments (30 secs - 5 mins) and propose \textbf{LongAudio}, a large and novel dataset for training ALMs on long audio captioning and question-answering tasks. Fine-tuning AF2 on LongAudio leads to exceptional performance on our proposed \textbf{LongAudioBench}, an expert annotated benchmark for evaluating ALMs on long audio understanding capabilities. We conduct extensive ablation studies to confirm the efficacy of our approach. All code and data will be open-sourced. </p>
            </div>
            
            <div class="section">
                <h2>Key Features</h2>
                <ul>
                    <li>Advanced audio reasoning and understanding</li>
                    <li>Supports long-audio (30 sec - 5 min) analysis</li>
                    <li>Outperforms larger models across 20+ benchmarks</li>
                    <li>Uses a custom CLAP model with enhanced training</li>
                    <li>Open-source with a demo available</li>
                </ul>
            </div>
    
            <div class="section">
                <img src="af2/audioskills_examples-1.png" alt="audioskills_exampls" width="75%" class="center">
            </div>
    
            <div class="section">
                <img src="af2/af2_clap-1.png" alt="CLAP" width="75%" class="center">
            </div>
    
            <div class="section">
                <img src="af2/long_audio_pipeline-1.png" alt="long audii" width="75%" class="center">
            </div>
    
            <div class="section">
                <h2>Benchmarks & Performance</h2>
                <p>AF2 achieves state-of-the-art accuracy across various benchmarks, including ClothoAQA, AudioCaps, MMAU, and LongAudioBench. It outperforms proprietary models while being significantly smaller.</p>
                <img src="af2/tab1.png" alt="Benchmark comparison graph">
                <img src="af2/tab2.png" alt="Performance evaluation chart" width="50%">
            </div>
        <div class="section">
            <h2>Examples</h2>
            <p>AF2 is trained on the novel <strong>LongAudio</strong> dataset, consisting of 260K+ AQA pairs, and <strong>LongAudioBench</strong>, an expert-verified evaluation benchmark for long audio understanding.</p>
            <table>
                <thead>
                    <tr>
                        <th>Audio</th>
                        <th>Question</th>
                        <th>Qwen2-Audio</th>
                        <th>GAMA</th>
                        <th>AudioFlamingo 2</th>
                    </tr>
                </thead>
                <tbody id="examplesTableBody">
                </tbody>
            </table>
        </div>
            
    <footer>
        <p>&copy; 2025 Audio Flamingo 2 | ICML Publication</p>
    </footer>
</body>
</html>
